{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM6513] Assignment 2: Text Classification with a Feedforward Network\n",
    "\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop a Feedforward neural network for text classification. \n",
    "\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "- Text processing methods for transforming raw text data into input vectors for your network  (**2 marks**)\n",
    "\n",
    "\n",
    "- A Feedforward network consisting of:\n",
    "    - **One-hot** input layer mapping words into an **Embedding weight matrix** (**2 marks**)\n",
    "    - **One hidden layer** computing the mean embedding vector of all words in input followed by a **ReLU activation function** (**2 marks**)\n",
    "    - **Output layer** with a **softmax** activation. (**2 marks**)\n",
    "\n",
    "\n",
    "- The Stochastic Gradient Descent (SGD) algorithm with **back-propagation** to learn the weights of your Neural network. Your algorithm should:\n",
    "    - Use (and minimise) the **Categorical Cross-entropy loss** function (**2 marks**)\n",
    "    - Perform a **Forward pass** to compute intermediate outputs (**5 marks**)\n",
    "    - Perform a **Backward pass** to compute gradients and update all sets of weights (**12 marks**)\n",
    "    - Implement and use **Dropout** after each hidden layer for regularisation (**4 marks**)\n",
    "\n",
    "\n",
    "\n",
    "- Discuss how did you choose hyperparameters? You can tune the learning rate (hint: choose small values), embedding size {e.g. 50, 300, 500}, the dropout rate {e.g. 0.2, 0.5} and the learning rate. Please use tables or graphs to show training and validation performance for each hyperparameter combination  (**5 marks**). \n",
    "\n",
    "\n",
    "\n",
    "- After training a model, plot the learning process (i.e. training and validation loss in each epoch) using a line plot and report accuracy. Does your model overfit, underfit or is about right? (**2 marks**).\n",
    "\n",
    "\n",
    "\n",
    "- Re-train your network by using pre-trained embeddings ([GloVe](https://nlp.stanford.edu/projects/glove/)) trained on large corpora. Instead of randomly initialising the embedding weights matrix, you should initialise it with the pre-trained weights. During training, you should not update them (i.e. weight freezing) and backprop should stop before computing gradients for updating embedding weights. Report results by performing hyperparameter tuning and plotting the learning process. Do you get better performance? (**7 marks**).\n",
    "\n",
    "\n",
    "\n",
    "- Extend you Feedforward network by adding more hidden layers (e.g. one more or two). How does it affect the performance? Note: You need to repeat hyperparameter tuning, but the number of combinations grows exponentially. Therefore, you need to choose a subset of all possible combinations (**8 marks**)\n",
    "\n",
    "\n",
    "- Provide well documented and commented code describing all of your choices. In general, you are free to make decisions about text processing (e.g. punctuation, numbers, vocabulary size) and hyperparameter values. We expect to see justifications and discussion for all of your choices (**5 marks**). \n",
    "\n",
    "\n",
    "\n",
    "- Provide efficient solutions by using Numpy arrays when possible. Executing the whole notebook with your code should not take more than 10 minutes on any standard computer (e.g. Intel Core i5 CPU, 8 or 16GB RAM) excluding hyperparameter tuning runs and loading the pretrained vectors. You can find tips in [Intro to Python for NLP](https://sheffieldnlp.github.io/com6513/assets/labs/a0_python_intro.pdf) (**2 marks**). \n",
    "\n",
    "\n",
    "\n",
    "### Data \n",
    "\n",
    "The data you will use for the task is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "### Pre-trained Embeddings\n",
    "\n",
    "You can download pre-trained GloVe embeddings trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) from [here](http://nlp.stanford.edu/data/glove.840B.300d.zip). No need to unzip, the file is large.\n",
    "\n",
    "### Save Memory\n",
    "\n",
    "To save RAM, when you finish each experiment you can delete the weights of your network using `del W` followed by Python's garbage collector `gc.collect()`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment2.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex`).\n",
    "\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy (excluding built-in softmax funtcions) and Pandas. You are **not allowed to use any third-party library** such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras, Pytorch etc.. You should mention if you've used Windows to write and test your code because we mostly use Unix based machines for marking (e.g. Ubuntu, MacOS). \n",
    "\n",
    "There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results is as important as the accuracy itself. \n",
    "\n",
    "This assignment will be marked out of 60. It is worth 60\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Fri, 23 Apr 2021** and it needs to be submitted via Blackboard. Standard departmental penalties for lateness will be applied. We use a range of strategies to **detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index)**, including Turnitin which helps detect plagiarism. Use of unfair means would result in getting a failing grade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used Windows to write and test my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:46.938040Z",
     "start_time": "2021-04-21T00:51:45.823658Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from time import localtime, strftime\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "import zipfile\n",
    "import gc\n",
    "from numpy.linalg import norm\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Raw texts into training and development data\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:46.969577Z",
     "start_time": "2021-04-21T00:51:46.940039Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./data_topic/train.csv', encoding = 'UTF-8', header = None)\n",
    "df2 = pd.read_csv('./data_topic/dev.csv', encoding = 'UTF-8', header = None)\n",
    "df3 = pd.read_csv('./data_topic/test.csv', encoding = 'UTF-8', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:46.985497Z",
     "start_time": "2021-04-21T00:51:46.972542Z"
    }
   },
   "outputs": [],
   "source": [
    "train_attri = df1.iloc[:, 1].tolist()\n",
    "train_label = df1.iloc[:, 0]\n",
    "dev_attri = df2.iloc[:, 1].tolist()\n",
    "dev_label = df2.iloc[:, 0]\n",
    "test_attri = df3.iloc[:, 1].tolist()\n",
    "test_label = df3.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input representations\n",
    "\n",
    "\n",
    "To train your Feedforward network, you first need to obtain input representations given a vocabulary. One-hot encoding requires large memory capacity. Therefore, we will instead represent documents as lists of vocabulary indices (each word corresponds to a vocabulary index). \n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of words. You should: \n",
    "- tokenise all texts into a list of unigrams (tip: you can re-use the functions from Assignment 1) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- remove unigrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of the top-N most frequent unigrams in the entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.001740Z",
     "start_time": "2021-04-21T00:51:46.987492Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i', 'if',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
    "              'but', 'not', 'there', 'no', 'does', 'not', 'so', 've', 'their',\n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.017930Z",
     "start_time": "2021-04-21T00:51:47.003739Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', \n",
    "                   stop_words=[], vocab=set()):\n",
    "    \n",
    "    tokenRE = re.compile(token_pattern)\n",
    "    \n",
    "    # first extract all unigrams by tokenising\n",
    "    x_uni = [w for w in tokenRE.findall(str(x_raw).lower(),) if w not in stop_words]\n",
    "    \n",
    "    # this is to store the ngrams to be returned\n",
    "    x = []\n",
    "\n",
    "    if ngram_range[0]==1:\n",
    "        x = x_uni\n",
    "        \n",
    "    # generate n-grams from the available unigrams x_uni\n",
    "    ngrams = []\n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "\n",
    "    # ignore unigrams\n",
    "        if n==1: continue\n",
    "\n",
    "        # pass a list of lists as an argument for zip\n",
    "        arg_list = [x_uni]+[x_uni[i:] for i in range(1, n)]\n",
    "\n",
    "        # extract tuples of n-grams using zip\n",
    "        # for bigram this should look: list(zip(x_uni, x_uni[1:]))\n",
    "        # align each item x[i] in x_uni with the next one x[i+1]. \n",
    "        # Note that x_uni and x_uni[1:] have different lenghts\n",
    "        # but zip ignores redundant elements at the end of the second list\n",
    "        # Alternatively, this could be done with for loops\n",
    "        x_ngram = list(zip(*arg_list))\n",
    "        ngrams.append(x_ngram)\n",
    "\n",
    "\n",
    "    for n in ngrams:\n",
    "        for t in n:\n",
    "            x.append(t)\n",
    "\n",
    "    if len(vocab)>0:\n",
    "        x = [w for w in x if w in vocab]\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.033778Z",
     "start_time": "2021-04-21T00:51:47.019698Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', \n",
    "              min_df=0, keep_topN=0, \n",
    "              stop_words=[]):\n",
    "    \n",
    "    \n",
    "    tokenRE = re.compile(token_pattern)\n",
    "    \n",
    "    df = Counter()\n",
    "    ngram_counts = Counter()\n",
    "    vocab = set()\n",
    "    \n",
    "    # interate through each raw text\n",
    "    for x in X_raw:\n",
    "        \n",
    "        x_ngram = extract_ngrams(x, ngram_range=ngram_range, token_pattern=token_pattern, \n",
    "                                 stop_words=stop_words)\n",
    "        \n",
    "        #update doc and ngram frequencies \n",
    "        df.update(list(set(x_ngram)))\n",
    "        ngram_counts.update(x_ngram)\n",
    "\n",
    "    # obtain a vocabulary as a set. \n",
    "    # Keep elements with doc frequency > minimum doc freq (min_df)\n",
    "    # Note that df contains all te\n",
    "    vocab = set([w for w in df if df[w]>=min_df])\n",
    "    \n",
    "    # keep the top N most freqent \n",
    "    if keep_topN>0:\n",
    "        vocab = set([w[0] for w in ngram_counts.most_common(keep_topN) \n",
    "                     if w[0] in vocab])\n",
    "      \n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.160098Z",
     "start_time": "2021-04-21T00:51:47.034779Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract vocabulary from training data.\n",
    "vocab = get_vocab(train_attri, ngram_range=(1,1), min_df=7, keep_topN=1000, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and word -> vocabulary id dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.175060Z",
     "start_time": "2021-04-21T00:51:47.161098Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_2_dict(result_from_vocab):\n",
    "    id_word = dict()\n",
    "    word_id = dict()\n",
    "    id = 1\n",
    "    for gram in result_from_vocab[0]:\n",
    "        id_word.update({id: gram})\n",
    "        word_id.update({gram: id})\n",
    "        id += 1\n",
    "    return id_word, word_id\n",
    "# Create vocabulary id -> word and word -> vocabulary id dictionaries.\n",
    "id2word, word2id = create_2_dict(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the list of unigrams  into a list of vocabulary indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing actual one-hot vectors into memory for all words in the entire data set is prohibitive. Instead, we will store word indices in the vocabulary and look-up the weight matrix. This is equivalent of doing a dot product between an one-hot vector and the weight matrix. \n",
    "\n",
    "First, represent documents in train, dev and test sets as lists of words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.300250Z",
     "start_time": "2021-04-21T00:51:47.177055Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Represent documents in train sets as lists of words in the vocabulary.   \n",
    "def docs_vocab(docs):\n",
    "    set_vocab=[]\n",
    "    for i in range(len(docs)):\n",
    "        new_row = []\n",
    "        # Extract unigrams of every document.\n",
    "        ngrams = extract_ngrams(docs[i], ngram_range=(1,1), stop_words=stop_words)\n",
    "        for word in ngrams:\n",
    "            # For every word in each document, if the word can be found in the vocab, \n",
    "            # then contain it to a new list.\n",
    "            if word in vocab[0]:\n",
    "                new_row.append(word)\n",
    "        # Store lists of words in the vocabulary in every document.\n",
    "        set_vocab.append(new_row)\n",
    "    return set_vocab\n",
    "# Represent documents in train, dev and test sets as lists of words in the vocabulary\n",
    "train_set_vocab = docs_vocab(train_attri)\n",
    "dev_set_vocab = docs_vocab(dev_attri)\n",
    "test_set_vocab = docs_vocab(test_attri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.316262Z",
     "start_time": "2021-04-21T00:51:47.301249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the longest sentence in train set is:  56\n",
      "The length of the longest sentence in dev set is:  31\n",
      "The length of the longest sentence in test set is:  50\n"
     ]
    }
   ],
   "source": [
    "# Get the max length of the longest document for creating indices matrix later.   \n",
    "max_len_train = np.max([len(text) for text in train_set_vocab])\n",
    "max_len_dev = np.max([len(text) for text in dev_set_vocab])\n",
    "max_len_test = np.max([len(text) for text in test_set_vocab])\n",
    "# Find the length of the longest sentence in train, dev and test set.\n",
    "print(\"The length of the longest sentence in train set is: \", max_len_train)\n",
    "print(\"The length of the longest sentence in dev set is: \", max_len_dev)\n",
    "print(\"The length of the longest sentence in test set is: \", max_len_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follow function can map one-hot input layer into an rmbedding weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.332165Z",
     "start_time": "2021-04-21T00:51:47.317208Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Represent documents in train sets as lists of words in the vocabulary.   \n",
    "def docs_indices(docs_v, width = 300):\n",
    "    set_indices=[]\n",
    "    for i in range(len(docs_v)):\n",
    "        new_row = []\n",
    "        for word in docs_v[i]:\n",
    "            # For every word in each sentence, add their indices in new list.\n",
    "            new_row.append(word2id[word])\n",
    "        # The length of each sentence is different, so add 0 to the list whose length is less than the maximum length (56).\n",
    "        new_row = new_row + [0] * (width - len(new_row))\n",
    "        # Store lists of words in the vocabulary in every document.\n",
    "        set_indices.append(new_row)\n",
    "    return set_indices\n",
    "\n",
    "# I will convert the list of unigrams into a list of vocabulary indices when I train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the labels `Y` for train, dev and test sets into arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.348124Z",
     "start_time": "2021-04-21T00:51:47.334161Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_train = train_label.to_numpy()\n",
    "Y_val = dev_label.to_numpy()\n",
    "Y_test = test_label.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "\n",
    "Your network should pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n",
    "\n",
    "where $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n",
    "\n",
    "Then $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n",
    "\n",
    "$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n",
    "\n",
    "Finally the hidden layer is passed to the output layer:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W) $$ \n",
    "where $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n",
    "\n",
    "During training, $\\mathbf{a}_1$ should be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n",
    "\n",
    "You can extend to a deeper architecture by passing a hidden layer to another one:\n",
    "\n",
    "$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i $$\n",
    "\n",
    "$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Training\n",
    "\n",
    "First we need to define the parameters of our network by initiliasing the weight matrices. For that purpose, you should implement the `network_weights` function that takes as input:\n",
    "\n",
    "- `vocab_size`: the size of the vocabulary\n",
    "- `embedding_dim`: the size of the word embeddings\n",
    "- `hidden_dim`: a list of the sizes of any subsequent hidden layers. Empty if there are no hidden layers between the average embedding and the output layer \n",
    "- `num_classes`: the number of the classes for the output layer\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: a dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers (hint: use numpy.random.uniform with from -0.1 to 0.1)\n",
    "\n",
    "Make sure that the dimensionality of each weight matrix is compatible with the previous and next weight matrix, otherwise you won't be able to perform forward and backward passes. Consider also using np.float32 precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.364120Z",
     "start_time": "2021-04-21T00:51:47.350119Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def network_weights(vocab_size=1000, embedding_dim=300, \n",
    "                    hidden_dim=[], num_classes=3, init_val = 0.5):\n",
    "    W = list()\n",
    "    # Create W with embedding_dim*num_classes dimension \n",
    "    # if there are no hidden layers between the average embedding and the output layer.\n",
    "    if len(hidden_dim) == 0:\n",
    "        layer = [{'weights':[np.float32(np.random.uniform(-1e-1, 1e-1)) for i in range(embedding_dim)]} for i in range(num_classes)]\n",
    "        W.append(layer)\n",
    "        return W\n",
    "    # Create W if there are hidden layers between the average embedding and the output layer.\n",
    "    else:\n",
    "        # W with embedding_dim*hidden_dim[0] dimension\n",
    "        layer = [{'weights':[np.float32(np.random.uniform(-1e-1, 1e-1)) for i in range(embedding_dim)]} for i in range(hidden_dim[0])]\n",
    "        W.append(layer)\n",
    "        # W with hidden_dim[j]*hidden_dim[j+1] dimension\n",
    "        for j in range(len(hidden_dim)-1):\n",
    "            layer = [{'weights':[np.float32(np.random.uniform(-1e-1, 1e-1)) for i in range(hidden_dim[j])]} for i in range(hidden_dim[j+1])]\n",
    "            W.append(layer)\n",
    "        # W with hidden_dim[-1]*num_classes dimension\n",
    "        layer = [{'weights':[np.float32(np.random.uniform(-1e-1, 1e-1)) for i in range(hidden_dim[-1])]} for i in range(num_classes)]\n",
    "        W.append(layer)\n",
    "        return W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.380037Z",
     "start_time": "2021-04-21T00:51:47.366074Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Weights = network_weights(vocab_size=3,embedding_dim=5,hidden_dim=[3], num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:31:57.970152Z",
     "start_time": "2020-04-01T10:31:57.966123Z"
    }
   },
   "source": [
    "Then you need to develop a `softmax` function (same as in Assignment 1) to be used in the output layer. \n",
    "\n",
    "It takes as input `z` (array of real numbers) and returns `sig` (the softmax of `z`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.396057Z",
     "start_time": "2021-04-21T00:51:47.382031Z"
    }
   },
   "outputs": [],
   "source": [
    "# To avoid the probably blow up exponents, first shift the highest value in array to zero, then compute the softmax.\n",
    "def softmax(z):\n",
    "    expo = np.exp(z-np.max(z))\n",
    "    expo_sum = np.sum(np.exp(z-np.max(z)))\n",
    "    sig = expo/expo_sum    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the categorical cross entropy loss by slightly modifying the function from Assignment 1 to depend only on the true label `y` and the class probabilities vector `y_preds`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.412156Z",
     "start_time": "2021-04-21T00:51:47.396992Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def categorical_loss(y, y_preds):\n",
    "    epsilon = 1e-10\n",
    "    l = -np.sum(y * np.log(y_preds + epsilon))/ y.shape[0]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:02:56.149535Z",
     "start_time": "2020-03-31T15:02:56.145738Z"
    }
   },
   "source": [
    "Then, implement the `relu` function to introduce non-linearity after each hidden layer of your network \n",
    "(during the forward pass): \n",
    "\n",
    "$$relu(z_i)= max(z_i,0)$$\n",
    "\n",
    "and the `relu_derivative` function to compute its derivative (used in the backward pass):\n",
    "\n",
    "  \n",
    "  relu_derivative($z_i$)=0, if $z_i$<=0, 1 otherwise.\n",
    "  \n",
    "\n",
    "\n",
    "Note that both functions take as input a vector $z$ \n",
    "\n",
    "Hint use .copy() to avoid in place changes in array z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.427321Z",
     "start_time": "2021-04-21T00:51:47.413096Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    zz = z.copy()\n",
    "    a = np.maximum(0, zz)    \n",
    "    return a\n",
    "    \n",
    "def relu_derivative(z):\n",
    "    dz = z.copy()\n",
    "    dz[dz <= 0] = 0\n",
    "    dz[dz > 0] = 1  \n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training you should also apply a dropout mask element-wise after the activation function (i.e. vector of ones with a random percentage set to zero). The `dropout_mask` function takes as input:\n",
    "\n",
    "- `size`: the size of the vector that we want to apply dropout\n",
    "- `dropout_rate`: the percentage of elements that will be randomly set to zeros\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `dropout_vec`: a vector with binary values (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.443268Z",
     "start_time": "2021-04-21T00:51:47.429091Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dropout_mask(size, dropout_rate):\n",
    "    dropout_vec = np.ones(size)\n",
    "    # Decide the number of 0 in vector.\n",
    "    dropout_vec[ :int(size*dropout_rate)] = 0\n",
    "    np.random.shuffle(dropout_vec)\n",
    "    return dropout_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.458972Z",
     "start_time": "2021-04-21T00:51:47.444014Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 1. 0. 0. 1. 1. 0.]\n",
      "[1. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(dropout_mask(10, 0.5))\n",
    "print(dropout_mask(10, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the `forward_pass` function that passes the input x through the network up to the output layer for computing the probability for each class using the weight matrices in `W`. The ReLU activation function should be applied on each hidden layer. \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `dropout_rate`: the dropout rate that is used to generate a random dropout mask vector applied after each hidden layer for regularisation.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `out_vals`: a dictionary of output values from each layer: h (the vector before the activation function), a (the resulting vector after passing h from the activation function), its dropout mask vector; and the prediction vector (probability for each class) from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.474938Z",
     "start_time": "2021-04-21T00:51:47.459970Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.2):\n",
    "    out_vals = {}\n",
    "    h_vecs = []\n",
    "    a_vecs = []\n",
    "    dropout_vecs = []\n",
    "    inputs = x\n",
    "    w_idx = 0\n",
    "    # Get all hidden layers\n",
    "    W_hidden = W[:-1]\n",
    "    # Get output layer\n",
    "    W_output = W[-1]\n",
    "    # Use ReLu function to introduce non-linearity after each hidden layer\n",
    "    for layer in W_hidden:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            # The vector before the activation function.\n",
    "            h_vecs = np.matmul(inputs, neuron['weights'])\n",
    "            # The resulting vector after passing h_vecs from the activation function.\n",
    "            neuron['output'] = relu(h_vecs)\n",
    "            a_vecs = neuron['output']\n",
    "            new_inputs.append(a_vecs)\n",
    "        inputs = new_inputs\n",
    "        # Generate and implement dropout mask vector for regularisation.\n",
    "        drop_vec = dropout_mask(len(inputs), dropout_rate)\n",
    "        inputs = np.multiply(inputs, drop_vec)\n",
    "        out_vals.update({'w'+str(w_idx): inputs})\n",
    "        w_idx += 1\n",
    "    # Use softmax function in output layer.\n",
    "    final_output = []\n",
    "    for neuron in W_output:\n",
    "        h_vecs = np.matmul(inputs, neuron['weights'])\n",
    "        neuron['output'] = h_vecs\n",
    "        a_vecs = neuron['output']\n",
    "        final_output.append(a_vecs)\n",
    "    out_vals.update({'w'+str(w_idx): softmax(final_output)})\n",
    "    return out_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward_pass` function computes the gradients and updates the weights for each matrix in the network from the output to the input. It takes as input \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `y`: the true label\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `out_vals`: a dictionary of output values from a forward pass.\n",
    "- `learning_rate`: the learning rate for updating the weights.\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: the updated weights of the network.\n",
    "\n",
    "Hint: the gradients on the output layer are similar to the multiclass logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.489925Z",
     "start_time": "2021-04-21T00:51:47.475927Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, y, W, out_vals, lr=0.001, freeze_emb=False):\n",
    "    # Transform dictionary W to array W for calculation later.\n",
    "    W_a = []\n",
    "    for n in range(len(W)):\n",
    "        w = []\n",
    "        for neuron in W[n]:\n",
    "            w.append(neuron['weights'])\n",
    "        # Transform every weight matrix to array and transpose it for calculation, then append to a new list.\n",
    "        W_a.append(np.array(w).T)\n",
    "    for i in reversed(range(len(W))):\n",
    "        # When it is output layer.\n",
    "        if i == (len(W)-1):\n",
    "            # Computer gradient of softmax layer with Cross-Entropy, \n",
    "            # which is the difference between the predicted probability- \n",
    "            # output of the network and the target probability output.\n",
    "            delta = (out_vals['w'+str(i)] - y).reshape(1, len(y))\n",
    "            # Gradient Clipping\n",
    "            if norm(delta) >1e+10:\n",
    "                delta = delta/norm(delta)\n",
    "            #print(norm(delta))\n",
    "            # Update weight\n",
    "            W_a[i] -= lr * np.dot(out_vals['w'+str(i-1)].reshape(len(out_vals['w'+str(i-1)]), 1), delta)\n",
    "        # When it is the hiddenlayer except the first one.\n",
    "        elif i != 0 and i != (len(W)-1):\n",
    "            delta = np.dot(delta, W_a[i+1].T) * relu_derivative(out_vals['w'+str(i)])\n",
    "            # Gradient Clipping\n",
    "            if norm(delta) >1e+10:\n",
    "                delta = delta/norm(delta)\n",
    "            #print(norm(delta))\n",
    "            W_a[i] -= lr * np.dot(out_vals['w'+str(i-1)].reshape(len(out_vals['w'+str(i-1)]), 1), delta)\n",
    "        # When it is the first layer.\n",
    "        else:\n",
    "            # Decide whether freezing the embedding matrix.\n",
    "            if freeze_emb == False:\n",
    "                delta = np.dot(delta, W_a[i+1].T) * relu_derivative(out_vals['w'+str(i)])\n",
    "                # Gradient Clipping\n",
    "                if norm(delta) >1e+10:\n",
    "                    delta = delta/norm(delta)\n",
    "                #print(norm(delta))\n",
    "                W_a[i] -= lr * np.dot(np.array(x).reshape(len(x), 1), delta)\n",
    "            if freeze_emb == True:\n",
    "                pass\n",
    "    # Transform array W to dictionary W for forward pass again.\n",
    "    W = []\n",
    "    for j in range(len(W_a)):\n",
    "        W_trans = []\n",
    "        for neuron in W_a[j].T:\n",
    "            neuron = neuron.astype(np.float32)\n",
    "            W_trans.append({'weights':list(neuron)})\n",
    "        W.append(W_trans)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.504890Z",
     "start_time": "2021-04-21T00:51:47.490888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot(y):\n",
    "    K=len(np.unique(y.tolist())) \n",
    "    eyes_mat=np.eye(K)           \n",
    "    y_onehot=np.ones((y.shape[0],K)) \n",
    "    for i in range(0,y.shape[0]):\n",
    "        y_onehot[i]=eyes_mat[y[i]-1]  # Update one-hot coder matrix accroding to the label value.\n",
    "    return y_onehot\n",
    "\n",
    "ytrue = one_hot(Y_train)\n",
    "ytrue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support back-propagation by using the `forward_pass` and `backward_pass` functions.\n",
    "\n",
    "The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `W`: the weights of the network (dictionary)\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `dropout`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated (to be used by the backward pass function).\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:51:47.520807Z",
     "start_time": "2021-04-21T00:51:47.505848Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev, Y_dev, lr=0.001, \n",
    "        dropout=0.2, epochs=5, tolerance=0.001, freeze_emb=False, \n",
    "        print_progress=True):\n",
    "    # Transform train and dev labels to one-hot form.\n",
    "    Y_tr_OH = one_hot(Y_tr)\n",
    "    Y_dev_OH = one_hot(Y_dev)\n",
    "    # Create two loss lists.\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    training_loss_history.append(0)\n",
    "    validation_loss_history.append(0)\n",
    "    weight = W.copy()\n",
    "    # Zip two lists for shuffle later\n",
    "    whole_data = list(zip(X_tr, Y_tr_OH.tolist()))\n",
    "    X = X_tr\n",
    "    Y = Y_tr_OH\n",
    "    for i in range(epochs):\n",
    "        y_tr_pred = []\n",
    "        y_dev_pred = []\n",
    "        # Shuffle data\n",
    "        random.shuffle(whole_data)\n",
    "        X, Y = zip(*whole_data)\n",
    "        Y = np.array(Y)\n",
    "        for j in range(len(X)):\n",
    "            # Forward pass.\n",
    "            output = forward_pass(X[j], weight, dropout)\n",
    "            #print(output)\n",
    "            # Backward pass.\n",
    "            weight = backward_pass(X[j], Y[j], weight, output)\n",
    "            #print(weight)\n",
    "        \n",
    "        # Use updated weights to predict train labels.  \n",
    "        for m in range(len(X)):\n",
    "            output_tr = forward_pass(X[m], weight, dropout)\n",
    "            y_tr_pred.append(output_tr['w'+str(len(weight)-1)])        \n",
    "        # Compute training loss is this epoch.\n",
    "        tr_loss = categorical_loss(Y, np.array(y_tr_pred))\n",
    "        training_loss_history.append(tr_loss)\n",
    "        \n",
    "        # Use updated weights to predict dev labels.\n",
    "        for k in range(len(X_dev)):\n",
    "            output_dev = forward_pass(X_dev[k], weight, dropout)\n",
    "            y_dev_pred.append(output_dev['w'+str(len(weight)-1)])\n",
    "        # Compute dev loss is this epoch.\n",
    "        dev_loss = categorical_loss(Y_dev_OH, np.array(y_dev_pred))\n",
    "        validation_loss_history.append(dev_loss)\n",
    "        \n",
    "        # Stop training if the difference between the current and \n",
    "        # previous validation loss is smaller than a threshold\n",
    "        if abs(validation_loss_history[-1]-validation_loss_history[-2]) < tolerance:\n",
    "            return weight, training_loss_history[1:], validation_loss_history[1:]    \n",
    "    return weight, training_loss_history[1:], validation_loss_history[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate your neural net. First, you need to define your network using the `network_weights` function followed by SGD with backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:52:25.133559Z",
     "start_time": "2021-04-21T00:51:47.521804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialise the weights of the network\n",
    "Weights = network_weights(embedding_dim=300, hidden_dim=[10], num_classes=3)\n",
    "# Get final embedding matrix\n",
    "X_train = docs_indices(train_set_vocab, 300)\n",
    "X_val = docs_indices(dev_set_vocab, 300)\n",
    "X_test = docs_indices(test_set_vocab, 300)\n",
    "\n",
    "W_final, loss_tr, dev_loss = SGD(X_train, Y_train,\n",
    "                                 Weights,\n",
    "                                 X_dev=X_val, \n",
    "                                 Y_dev=Y_val,\n",
    "                                 lr=2e-8, \n",
    "                                 dropout=0.5,\n",
    "                                 freeze_emb=False,\n",
    "                                 tolerance=0.0001,\n",
    "                                 epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:52:25.276466Z",
     "start_time": "2021-04-21T00:52:25.134560Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x27998715cd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array(range(1, len(loss_tr)+1))\n",
    "plt.plot(x, loss_tr, label='training loss')\n",
    "plt.plot(x, dev_loss, label='validation loss')\n",
    "plt.title(\"Training and Validation History per Epoch\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss value\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above figure, the degree of overfitting of the model is low, because the two curves overlap many times, and the difference between the train loss and the validation loss set is small. Meanwhile, the model underfits because the two curves are still decreasing without convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:52:25.703453Z",
     "start_time": "2021-04-21T00:52:25.277464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33555555555555555\n",
      "Precision: 0.2628762313801684\n",
      "Recall: 0.33555555555555555\n",
      "F1-Score: 0.17724460646313966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Get predicted label weights of test set.\n",
    "y_test_pred = []\n",
    "for i in range(len(X_test)):\n",
    "    output_test = forward_pass(X_test[i], W_final, dropout_rate=0.0)\n",
    "    y_test_pred.append(output_test['w'+str(len(W_final)-1)])\n",
    "# Convert label weights to the real label class(1,2 or 3).\n",
    "y_test_pred_int = []\n",
    "for label in y_test_pred:\n",
    "    y_test_pred_int.append(np.argmax(label)+1)\n",
    "    \n",
    "print('Accuracy:', accuracy_score(Y_test,y_test_pred_int))\n",
    "print('Precision:', precision_score(Y_test,y_test_pred_int,average='macro'))\n",
    "print('Recall:', recall_score(Y_test,y_test_pred_int,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_test,y_test_pred_int,average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best hyperparameters, I chose different embbending size{56, 300, 500}, dropout rate{0.2, 0.5}, learning rate{1e-6, 1e-7, 1e-8, 1e-9} and dimension of hidden layer{20, 10}. For each model, I trained 10 epoches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:52:25.718178Z",
     "start_time": "2021-04-21T00:52:25.704494Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Code for hyperparameters tunning.\n",
    "# # Choose hyperparameters.\n",
    "# emb_size = 500\n",
    "# learning_rate = 1e-6\n",
    "# dropout_rate = 0.5\n",
    "# # Initialse embedding matrix and weight matrix.\n",
    "# X_train_tune = docs_indices(train_set_vocab, emb_size)\n",
    "# X_val_tune = docs_indices(dev_set_vocab, emb_size)\n",
    "# Weights_tune = network_weights(embedding_dim=emb_size, hidden_dim=[10], num_classes=3)\n",
    "# # Train model, each model has a 20 dimension hidden layer and trains 10 epoches.\n",
    "# W_final_tune, loss_tr_tune, dev_loss_tune = SGD(X_train_tune, Y_train,\n",
    "#                                  Weights_tune,\n",
    "#                                  X_dev=X_val_tune, \n",
    "#                                  Y_dev=Y_val,\n",
    "#                                  lr=learning_rate, \n",
    "#                                  dropout=dropout_rate,\n",
    "#                                  freeze_emb=False,\n",
    "#                                  tolerance=0.0001,\n",
    "#                                  epochs=10)\n",
    "\n",
    "# # Get predicted label weights of training and validation set.\n",
    "# y_train_pred = []\n",
    "# y_val_pred = []\n",
    "# for i in range(len(X_train_tune)):\n",
    "#     output_train = forward_pass(X_train_tune[i], W_final_tune, dropout_rate=0.0)\n",
    "#     y_train_pred.append(output_train['w'+str(len(W_final_tune)-1)])\n",
    "# for i in range(len(X_val_tune)):\n",
    "#     output_val = forward_pass(X_val_tune[i], W_final_tune, dropout_rate=0.0)\n",
    "#     y_val_pred.append(output_val['w'+str(len(W_final_tune)-1)])\n",
    "\n",
    "# # Convert label weights to the real label class(1,2 or 3).\n",
    "# y_train_pred_int = []\n",
    "# y_val_pred_int = []\n",
    "# for label in y_train_pred:\n",
    "#     y_train_pred_int.append(np.argmax(label)+1)\n",
    "# for label in y_val_pred:\n",
    "#     y_val_pred_int.append(np.argmax(label)+1)\n",
    "    \n",
    "# # Compute F1-score of the model on training and validation set.\n",
    "# print('Accuracy on training set:', accuracy_score(Y_train,y_train_pred_int))\n",
    "# print('Accuracy on validation set:', accuracy_score(Y_val,y_val_pred_int))\n",
    "\n",
    "# del W_final_tune\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The form of the result is: \"training accuracy/validation accuracy\". The final learning rate value of the model is obtained by more attempts. The 4 learning rate values in the following table are just \"boundaries\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of hidden layer: 20\n",
    "\n",
    "|Embedding size  |Dropout rate  | lr = 1e-6  | lr = 1e-7  | lr = 1e-8  | lr = 1e-9  |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| 56 |0.2   |0.333/0.333   |0.333/0.333   |0.333/0.340   |0.333/0.333   |\n",
    "| 56 |0.5   |0.333/0.333   |0.349/0.333   |0.333/0.333   |0.333/0.333   |\n",
    "| 300|0.2   |0.333/0.333   |0.333/0.333   |0.333/0.346   |0.338/0.326   |\n",
    "| 300|0.5   |0.338/0.333   |0.333/0.333   |0.366/0.340   |0.333/0.333   |\n",
    "| 500|0.2   |0.323/0.333   |0.333/0.333   |0.340/0.333   |0.333/0.333   |\n",
    "| 500|0.5   |0.333/0.333   |0.326/0.333   |0.333/0.340   |0.336/0.333   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of hidden layer: 10\n",
    "\n",
    "|Embedding size  |Dropout rate  | lr = 1e-6  | lr = 1e-7  | lr = 1e-8  | lr = 1e-9  |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| 56 |0.2   |0.357/0.340   |0.353/0.326   |0.333/0.333   |0.345/0.333   |\n",
    "| 56 |0.5   |0.333/0.333   |0.334/0.333   |0.333/0.333   |0.332/0.333   |\n",
    "| 300|0.2   |0.333/0.333   |0.333/0.333   |0.333/0.333   |0.343/0.334   |\n",
    "| 300|0.5   |0.338/0.333   |0.333/0.333   |0.382/0.333   |0.333/0.333   |\n",
    "| 500|0.2   |0.333/0.333   |0.343/0.326   |0.333/0.333   |0.333/0.333   |\n",
    "| 500|0.5   |0.333/0.333   |0.333/0.333   |0.326/0.343   |0.336/0.333   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the table above, the model have better performance with 10 hidden layer size, 300 embedding size, 0.5 droprate and 1e-8 learning rate, so I choose learning rate around 1e-8 for finding best hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained Embeddings\n",
    "\n",
    "Now re-train the network using GloVe pre-trained embeddings. You need to modify the `backward_pass` function above to stop computing gradients and updating weights of the embedding matrix.\n",
    "\n",
    "Use the function below to obtain the embedding martix for your vocabulary. Generally, that should work without any problem. If you get errors, you can modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:52:25.734209Z",
     "start_time": "2021-04-21T00:52:25.720173Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_glove_embeddings(f_zip, f_txt, word2id, emb_size=300):\n",
    "    \n",
    "    w_emb = np.zeros((len(word2id), emb_size))\n",
    "    \n",
    "    with zipfile.ZipFile(f_zip) as z:\n",
    "        with z.open(f_txt) as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                word = line.split()[0]\n",
    "                     \n",
    "                if word in vocab[0]:\n",
    "                    emb = np.array(line.strip('\\n').split()[1:]).astype(np.float32)\n",
    "                    w_emb[word2id[word]-1] +=emb\n",
    "    return w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:53:55.263216Z",
     "start_time": "2021-04-21T00:52:25.736133Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_glove = get_glove_embeddings(\"glove.840B.300d.zip\",\"glove.840B.300d.txt\",word2id, emb_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:53:55.279103Z",
     "start_time": "2021-04-21T00:53:55.263937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_glove.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialise the weights of your network using the `network_weights` function. Second, replace the weigths of the embedding matrix with `w_glove`. Finally, train the network by freezing the embedding weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:53:55.578303Z",
     "start_time": "2021-04-21T00:53:55.280099Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate indice matrix which width size equal the size of vocab(1000)\n",
    "X_train_1000 = docs_indices(train_set_vocab, 1000)\n",
    "X_val_1000 = docs_indices(dev_set_vocab, 1000)\n",
    "X_test_1000 = docs_indices(test_set_vocab, 1000)\n",
    "# Transform to array for np.dot\n",
    "X_train_1000_array = np.array(X_train_1000)\n",
    "X_val_1000_array = np.array(X_val_1000)\n",
    "X_test_1000_array = np.array(X_test_1000)\n",
    "# Get final embedding matrix for next two part.\n",
    "X_train_glove = np.dot(X_train_1000_array, w_glove).tolist()\n",
    "X_val_glove = np.dot(X_val_1000_array, w_glove).tolist()\n",
    "X_test_glove = np.dot(X_test_1000_array, w_glove).tolist()\n",
    "\n",
    "X_train_deep = X_train_glove.copy()\n",
    "X_val_deep = X_val_glove.copy()\n",
    "X_test_deep = X_test_glove.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:54:57.141506Z",
     "start_time": "2021-04-21T00:53:55.579299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialise the weights of the network\n",
    "Weights_glove = network_weights(embedding_dim=300, hidden_dim=[20], num_classes=3)\n",
    "W_final_glove, loss_tr_glove, dev_loss_glove = SGD(X_train_glove, Y_train,\n",
    "                                                 Weights_glove,\n",
    "                                                 X_dev=X_val_glove, \n",
    "                                                 Y_dev=Y_val,\n",
    "                                                 lr=8e-9, \n",
    "                                                 dropout=0.5,\n",
    "                                                 freeze_emb=True,\n",
    "                                                 tolerance=0.0001,\n",
    "                                                 epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:54:57.265248Z",
     "start_time": "2021-04-21T00:54:57.142230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x27998f2d3a0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array(range(1, len(loss_tr_glove)+1))\n",
    "plt.plot(x, loss_tr_glove, label='training loss')\n",
    "plt.plot(x, dev_loss_glove, label='validation loss')\n",
    "plt.title(\"Training and Validation History per Epoch\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss value\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above figure, the degree of overfitting of the model is low, because the two curves overlap many times, and the difference between the train loss and the validation loss set is small. Meanwhile, the model underfits because the two curves are still decreasing without convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:54:57.914245Z",
     "start_time": "2021-04-21T00:54:57.267012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34\n",
      "Precision: 0.5006270378730875\n",
      "Recall: 0.34\n",
      "F1-Score: 0.18624924605291582\n"
     ]
    }
   ],
   "source": [
    "# Get predicted label weights of test set.\n",
    "y_test_glove_pred = []\n",
    "for i in range(len(X_test_glove)):\n",
    "    output_glove_test = forward_pass(X_test_glove[i], W_final_glove, dropout_rate=0.0)\n",
    "    y_test_glove_pred.append(output_glove_test['w'+str(len(W_final_glove)-1)])\n",
    "# Convert label weights to the real label class(1,2 or 3).\n",
    "y_test_glove_pred_int = []\n",
    "for label in y_test_glove_pred:\n",
    "    y_test_glove_pred_int.append(np.argmax(label)+1)\n",
    "    \n",
    "print('Accuracy:', accuracy_score(Y_test,y_test_glove_pred_int))\n",
    "print('Precision:', precision_score(Y_test,y_test_glove_pred_int,average='macro'))\n",
    "print('Recall:', recall_score(Y_test,y_test_glove_pred_int,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_test,y_test_glove_pred_int,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, compared to the first model, the performance of the model with pre-trained embeddings has been improved a little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best hyperparameters, I choose different dropout rate{0.2, 0.5}, learning rate{1e-6, 1e-7, 1e-8, 1e-9}(embedding size is fixed as 300) and dimension of hidden layer{50, 20}. For each model, I trained 10 epoches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:54:57.929219Z",
     "start_time": "2021-04-21T00:54:57.915243Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Code for hyperparameters tunning.\n",
    "# # Choose hyperparameters.\n",
    "# emb_size = 300\n",
    "# learning_rate = 1e-6\n",
    "# dropout_rate = 0.5\n",
    "# # Initialse weight matrix (embedding matrix is already initialised).\n",
    "# Weights_glove_tune = network_weights(embedding_dim=emb_size, hidden_dim=[20], num_classes=3)\n",
    "# # Train model, each model has a 20 dimension hidden layer and trains 10 epoches.\n",
    "# W_glove_tune, loss_tr_tune, dev_loss_tune = SGD(X_train_glove, Y_train,\n",
    "#                                              Weights_glove_tune,\n",
    "#                                              X_dev=X_val_glove, \n",
    "#                                              Y_dev=Y_val,\n",
    "#                                              lr=learning_rate, \n",
    "#                                              dropout=dropout_rate,\n",
    "#                                              freeze_emb=True,\n",
    "#                                              tolerance=0.0001,\n",
    "#                                              epochs=10)\n",
    "\n",
    "# # Get predicted label weights of training and validation set.\n",
    "# y_train_glove_pred = []\n",
    "# y_val_glove_pred = []\n",
    "# for i in range(len(X_train_glove)):\n",
    "#     output_glove_train = forward_pass(X_train_glove[i], W_glove_tune, dropout_rate=0.0)\n",
    "#     y_train_glove_pred.append(output_glove_train['w'+str(len(W_glove_tune)-1)])\n",
    "# for i in range(len(X_val_glove)):\n",
    "#     output_glove_val = forward_pass(X_val_glove[i], W_glove_tune, dropout_rate=0.0)\n",
    "#     y_val_glove_pred.append(output_glove_val['w'+str(len(W_glove_tune)-1)])\n",
    "    \n",
    "# # Convert label weights to the real label class(1,2 or 3).\n",
    "# y_train_glove_pred_int = []\n",
    "# y_val_glove_pred_int = []\n",
    "# for label in y_train_glove_pred:\n",
    "#     y_train_glove_pred_int.append(np.argmax(label)+1)\n",
    "# for label in y_val_glove_pred:\n",
    "#     y_val_glove_pred_int.append(np.argmax(label)+1)\n",
    "    \n",
    "# # Compute F1-score of the model on training and validation set.\n",
    "# print('Accuracy on training set:', accuracy_score(Y_train,y_train_glove_pred_int))\n",
    "# print('Accuracy on validation set:', accuracy_score(Y_val,y_val_glove_pred_int))\n",
    "\n",
    "# del W_glove_tune\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The form of the result is: \"training accuracy/validation accuracy\". The final learning rate value of the model is obtained by more attempts. The 4 learning rate values in the following table are just \"boundaries\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of hidden layer: 50\n",
    "\n",
    "|Dropout rate  | lr = 1e-6  | lr = 1e-7  | lr = 1e-8  | lr = 1e-9  |\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|0.2   |0.333/0.333   |0.334/0.333   |0.333/0.333   |0.334/0.333   |\n",
    "|0.5   |0.326/0.333   |0.334/0.333   |0.334/0.333   |0.333/0.333   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of hidden layer: 20\n",
    "\n",
    "|Dropout rate  | lr = 1e-6  | lr = 1e-7  | lr = 1e-8  | lr = 1e-9  |\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|0.2   |0.333/0.333   |0.333/0.333   |0.343/0.333   |0.333/0.333   |\n",
    "|0.5   |0.333/0.333   |0.333/0.340   |0.354/0.333   |0.333/0.333   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "As can be seen in the table above, the model have better performance with 20 hidden layer size, 0.5 droprate and 1e-8 learning rate, so I choose learning rate around 1e-8 for finding best hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to support deeper architectures \n",
    "\n",
    "Extend the network to support back-propagation for more hidden layers. You need to modify the `backward_pass` function above to compute gradients and update the weights between intermediate hidden layers. Finally, train and evaluate a network with a deeper architecture. Do deeper architectures increase performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:58:51.764619Z",
     "start_time": "2020-04-02T14:58:47.483690Z"
    },
    "scrolled": true
   },
   "source": [
    "As the teacher mentioned in the group chat, I use pre-trained embeddings(GloVe) to train this model (embedding size is fixed as 300). I use two hidden layers in network. The 'backward_pass' function already support multi-layers network so I do not need to create a new function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:57:31.952066Z",
     "start_time": "2021-04-21T00:54:57.931216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialise the weights of the network.\n",
    "Weights_deep = network_weights(embedding_dim=300, hidden_dim=[50, 10], num_classes=3)\n",
    "W_final_deep, loss_tr_deep, dev_loss_deep = SGD(X_train_deep, Y_train,\n",
    "                                                 Weights_deep,\n",
    "                                                 X_dev=X_val_deep, \n",
    "                                                 Y_dev=Y_val,\n",
    "                                                 lr=8e-9, \n",
    "                                                 dropout=0.5,\n",
    "                                                 freeze_emb=True,\n",
    "                                                 tolerance=0.0001,\n",
    "                                                 epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:57:32.077767Z",
     "start_time": "2021-04-21T00:57:31.953083Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2799d2d3ca0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array(range(1, len(loss_tr_deep)+1))\n",
    "plt.plot(x, loss_tr_deep, label='training loss')\n",
    "plt.plot(x, dev_loss_deep, label='validation loss')\n",
    "plt.title(\"Training and Validation History per Epoch\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss value\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above figure, the degree of overfitting of the model is low, because the two curves overlap many times, and the difference between the train loss and the validation loss set is small. Meanwhile, the model underfits because the two curves are still decreasing without convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:57:33.684499Z",
     "start_time": "2021-04-21T00:57:32.078772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33444444444444443\n",
      "Precision: 0.2780252412769117\n",
      "Recall: 0.3344444444444445\n",
      "F1-Score: 0.1691524136990643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Get predicted label weights of test set.\n",
    "y_test_deep_pred = []\n",
    "for i in range(len(X_test_deep)):\n",
    "    output_deep_test = forward_pass(X_test_deep[i], W_final_deep, dropout_rate=0.0)\n",
    "    y_test_deep_pred.append(output_deep_test['w'+str(len(W_final_deep)-1)])\n",
    "# Convert label weights to the real label class(1,2 or 3).\n",
    "y_test_deep_pred_int = []\n",
    "for label in y_test_deep_pred:\n",
    "    y_test_deep_pred_int.append(np.argmax(label)+1)\n",
    "    \n",
    "print('Accuracy:', accuracy_score(Y_test,y_test_deep_pred_int))\n",
    "print('Precision:', precision_score(Y_test,y_test_deep_pred_int,average='macro'))\n",
    "print('Recall:', recall_score(Y_test,y_test_deep_pred_int,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_test,y_test_deep_pred_int,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the above model (pre-trained embeddings with one hidden layer), the performance of the model with deeper architecture has dropped a little. So I can say that in my case, the deeper network did not have positive impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best hyperparameters, I choose different dropout rate{0.2, 0.5},learning rate{1e-6, 1e-7, 1e-8} and hidden layer dimension {[100, 20], [100, 10], [50, 20], [50, 10]}.  For each model, I trained 10 epoches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T00:57:33.699852Z",
     "start_time": "2021-04-21T00:57:33.685512Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Code for hyperparameters tunning.\n",
    "# # Choose hyperparameters.\n",
    "# emb_size = 300\n",
    "# learning_rate = 1e-8\n",
    "# dropout_rate = 0.5\n",
    "# # Initialse weight matrix (embedding matrix is already initialised).\n",
    "# Weights_deep_tune = network_weights(embedding_dim=emb_size, hidden_dim=[50, 10], num_classes=3)\n",
    "# # Train model, each model has a 20 dimension hidden layer and trains 10 epoches.\n",
    "# W_deep_tune, loss_tr_deep_tune, dev_loss_deep_tune = SGD(X_train_deep, Y_train,\n",
    "#                                                          Weights_deep_tune,\n",
    "#                                                          X_dev=X_val_deep, \n",
    "#                                                          Y_dev=Y_val,\n",
    "#                                                          lr=learning_rate, \n",
    "#                                                          dropout=dropout_rate,\n",
    "#                                                          freeze_emb=True,\n",
    "#                                                          tolerance=0.0001,\n",
    "#                                                          epochs=10)\n",
    "\n",
    "# # Get predicted label weights of training and validation set.\n",
    "# y_train_deep_pred = []\n",
    "# y_val_deep_pred = []\n",
    "# for i in range(len(X_train_deep)):\n",
    "#     output_deep_train = forward_pass(X_train_deep[i], W_deep_tune, dropout_rate=0.0)\n",
    "#     y_train_deep_pred.append(output_deep_train['w'+str(len(W_deep_tune)-1)])\n",
    "# for i in range(len(X_val_deep)):\n",
    "#     output_deep_val = forward_pass(X_val_deep[i], W_deep_tune, dropout_rate=0.0)\n",
    "#     y_val_deep_pred.append(output_deep_val['w'+str(len(W_deep_tune)-1)])\n",
    "    \n",
    "# # Convert label weights to the real label class(1,2 or 3).\n",
    "# y_train_deep_pred_int = []\n",
    "# y_val_deep_pred_int = []\n",
    "# for label in y_train_deep_pred:\n",
    "#     y_train_deep_pred_int.append(np.argmax(label)+1)\n",
    "# for label in y_val_deep_pred:\n",
    "#     y_val_deep_pred_int.append(np.argmax(label)+1)\n",
    "    \n",
    "# # Compute F1-score of the model on training and validation set.\n",
    "# print('Accuracy on training set:', accuracy_score(Y_train,y_train_deep_pred_int))\n",
    "# print('Accuracy on validation set:', accuracy_score(Y_val,y_val_deep_pred_int))\n",
    "\n",
    "# del W_deep_tune\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The form of the result is: \"training accuracy/validation accuracy\". The final learning rate value of the model is obtained by more attempts. The 3 learning rate values in the following table are just \"boundaries\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of hidden layer: [100, 20]\n",
    "\n",
    "|Dropout rate  | lr = 1e-6  | lr = 1e-7  | lr = 1e-8  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|0.2   |0.333/0.333   |0.333/0.333   |0.334/0.333   |\n",
    "|0.5   |0.333/0.333   |0.334/0.333   |0.333/0.333   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of hidden layer: [100, 10]\n",
    "\n",
    "|Dropout rate  | lr = 1e-6  | lr = 1e-7  | lr = 1e-8  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|0.2   |0.334/0.333   |0.333/0.333   |0.334/0.333   |\n",
    "|0.5   |0.326/0.333   |0.333/0.334   |0.333/0.333   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of hidden layer: [50, 20]\n",
    "\n",
    "|Dropout rate  | lr = 1e-6  | lr = 1e-7  | lr = 1e-8  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|0.2   |0.333/0.334   |0.326/0.333   |0.333/0.333   |\n",
    "|0.5   |0.333/0.333   |0.333/0.340   |0.333/0.333   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of hidden layer: [50, 10]\n",
    "\n",
    "|Dropout rate  | lr = 1e-6  | lr = 1e-7  | lr = 1e-8  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|0.2   |0.326/0.333   |0.340/0.333   |0.343/0.333   |\n",
    "|0.5   |0.333/0.333   |0.333/0.333   |0.363/0.340   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the table above, the model have better performance with [50, 10] hidden layers size, 0.5 droprate and 1e-8 learning rate, so I choose learning rate around 1e-8 for finding best hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results\n",
    "\n",
    "Add your final results here:\n",
    "\n",
    "| Model | Precision  | Recall  | F1-Score  | Accuracy\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| Average Embedding  |0.262   |0.335   |0.177   |0.335   |\n",
    "| Average Embedding (Pre-trained)  |0.500   |0.340   |0.186   |0.340   |\n",
    "| Average Embedding (Pre-trained) + X hidden layers    |0.278   |0.334   |0.169   |0.334   |\n",
    "\n",
    "\n",
    "Please discuss why your best performing model is better than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "In my case, the Average Embedding (Pre-trained) model has the best performance. And I think the reason is that:\n",
    "\n",
    "1. Compared to average embedding, Pre-train embedding matrix is carried out on a huge text corpus and can learn general language representations, so it can provide better model initialization, which usually leads to better generalization performance. At the same time, it is regarded as a kind of regularization to avoid overfitting to small data.\n",
    "\n",
    "2. With regard to the model with deeper architexture, it is generally believed that increasing the number of hidden layers can reduce network errors and improve accuracy, but this will also complicate the network, thereby increasing the training time of the network and the tendency of \"overfitting\". In my case, the model with one hidden layer have better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
